{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0397f35b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't instantiate abstract class OpenAI without an implementation for abstract method '_prepare_chat_with_tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     28\u001b[39m OpenAI._prepare_chat_with_tools = \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     29\u001b[39m _ = OpenAI._prepare_chat_with_tools()\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m Settings.llm = \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt-3.5-turbo\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m Settings.embed_model = OpenAIEmbedding(model=\u001b[33m\"\u001b[39m\u001b[33mtext-embedding-ada-002\u001b[39m\u001b[33m\"\u001b[39m, api_key=OPEN_API_KEY)\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Dummy call to _prepare_chat_with_tools to bypass error\u001b[39;00m\n\u001b[32m     33\u001b[39m \n\u001b[32m     34\u001b[39m \n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m#Defiine Summary Index and Vector Index\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: Can't instantiate abstract class OpenAI without an implementation for abstract method '_prepare_chat_with_tools'"
     ]
    }
   ],
   "source": [
    "\n",
    "OPEN_API_KEY=''\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "#Load data\n",
    "\n",
    "\n",
    "file_path = '/workspaces/codespaces-jupyter/files/metagpt.pdf'\n",
    "\n",
    "\n",
    "#Define LLM and embedding model\n",
    "import os\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "documents = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
    "\n",
    "sentence_splitter=SentenceSplitter(chunk_size=1024)\n",
    "nodes=sentence_splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "#Define LLM and Embedding model\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\", api_key=OPEN_API_KEY)\n",
    "# Dummy call to _prepare_chat_with_tools to bypass error\n",
    "\n",
    "\n",
    "#Defiine Summary Index and Vector Index\n",
    "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
    "summary_index = SummaryIndex(nodes=nodes)\n",
    "vector_index = VectorStoreIndex(nodes=nodes)\n",
    "\n",
    "#Define Query Engines and Set Metadata\n",
    "summary_query_engine = summary_index.as_query_engine(response_mode='tree_summarize', use_async=True)\n",
    "vector_query_engine = vector_index.as_query_engine()\n",
    "\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "summary_tool = QueryEngineTool.from_defaults(query_engine=summary_query_engine, description=\"Summarization questions about the document\")\n",
    "vector_tool = QueryEngineTool.from_defaults(query_engine=vector_query_engine, description=\"Retrieving specific content from the document\")\n",
    "\n",
    "#Define Router query engine\n",
    "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "\n",
    "query_engine=RouterQueryEngine(selectors=LLMSingleSelector.from_defaults(),query_engine_tools=[summary_tool,vector_tool],verbose=True)\n",
    "\n",
    "#Define Agent\n",
    "response=query_engine.query(\"What is the document about?\")\n",
    "print(str(response))\n",
    "print(len(response.source_nodes))\n",
    "\n",
    "vector_response=query_engine.query(\"How do agents work?\")\n",
    "\n",
    "print(str(vector_response))\n",
    "print(len(vector_response.source_nodes))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
